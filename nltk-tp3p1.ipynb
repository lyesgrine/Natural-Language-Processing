{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"POS Tagging is a process to mark up the words in text format for a particular part of a speech, based on its definition and context.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POS',\n",
       " 'Tagging',\n",
       " 'is',\n",
       " 'a',\n",
       " 'process',\n",
       " 'to',\n",
       " 'mark',\n",
       " 'up',\n",
       " 'the',\n",
       " 'words',\n",
       " 'in',\n",
       " 'text',\n",
       " 'format',\n",
       " 'for',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'speech',\n",
       " ',',\n",
       " 'based',\n",
       " 'on',\n",
       " 'its',\n",
       " 'definition',\n",
       " 'and',\n",
       " 'context',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POS',\n",
       " 'Tagging',\n",
       " 'is',\n",
       " 'a',\n",
       " 'process',\n",
       " 'to',\n",
       " 'mark',\n",
       " 'up',\n",
       " 'the',\n",
       " 'words',\n",
       " 'in',\n",
       " 'text',\n",
       " 'format',\n",
       " 'for',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'speech',\n",
       " 'based',\n",
       " 'on',\n",
       " 'its',\n",
       " 'definition',\n",
       " 'and',\n",
       " 'context']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sw = stopwords.words('english')\n",
    "en_sw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_sw = stopwords.words('arabic')\n",
    "ar_sw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en_sw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m clean_tokens \u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43men_sw\u001b[49m:\n\u001b[0;32m      4\u001b[0m         clean_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m      5\u001b[0m clean_tokens\n",
      "\u001b[1;31mNameError\u001b[0m: name 'en_sw' is not defined"
     ]
    }
   ],
   "source": [
    "clean_tokens =[]\n",
    "for token in tokens:\n",
    "    if token not in en_sw:\n",
    "        clean_tokens.append(token)\n",
    "clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tokens = tknzr.tokenize(tweet)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a text written.',\n",
       " 'It uses U.S. english to illustrate sentence tokenization.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = 'This is a text written. It uses U.S. english to illustrate sentence tokenization.'\n",
    "sent_tokens = sent_tokenize(sentences)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading pos_tag: Package 'pos_tag' not found in\n",
      "[nltk_data]     index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('pos_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('cooool', 'JJ'),\n",
       " ('#dummysmiley', 'NN'),\n",
       " (':', ':'),\n",
       " (':-)', 'JJ'),\n",
       " (':-P', 'JJ'),\n",
       " ('<3', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('some', 'DT'),\n",
       " ('arrows', 'NNS'),\n",
       " ('<', 'VBP'),\n",
       " ('>', 'JJ'),\n",
       " ('->', 'CD'),\n",
       " ('<--', 'JJ')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = pos_tag(tokens)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DET'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('cooool', 'ADJ'),\n",
       " ('#dummysmiley', 'NOUN'),\n",
       " (':', '.'),\n",
       " (':-)', 'ADJ'),\n",
       " (':-P', 'ADJ'),\n",
       " ('<3', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('some', 'DET'),\n",
       " ('arrows', 'NOUN'),\n",
       " ('<', 'VERB'),\n",
       " ('>', 'ADJ'),\n",
       " ('->', 'NUM'),\n",
       " ('<--', 'ADJ')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = pos_tag(tokens, tagset='universal')\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " '.',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADJ',\n",
       " 'NUM',\n",
       " 'ADJ']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_list =[]\n",
    "for tag in tags:\n",
    "    tags_list.append(tag[1])\n",
    "tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tags_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m counts \u001b[38;5;241m=\u001b[39m Counter(tag \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtags_list\u001b[49m)\n\u001b[0;32m      2\u001b[0m counts\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tags_list' is not defined"
     ]
    }
   ],
   "source": [
    "counts = Counter(tag for tag in tags_list)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ADJ': 5,\n",
       "         'DET': 3,\n",
       "         'NOUN': 3,\n",
       "         'VERB': 2,\n",
       "         '.': 1,\n",
       "         'CONJ': 1,\n",
       "         'NUM': 1})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = Counter(tag for token,tag in tags)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster  = LancasterStemmer()\n",
    "\n",
    "porter_stem = porter.stem('probably')\n",
    "porter_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prob'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stem = lancaster.stem('probably')\n",
    "lancaster_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chang\n",
      "troubl\n",
      "troubl\n",
      "cat\n",
      "charcter\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem('changes'))\n",
    "print(porter.stem('troubling'))\n",
    "print(porter.stem('troubled'))\n",
    "print(porter.stem('cats'))\n",
    "print(porter.stem('charcterization'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chang\n",
      "troubl\n",
      "troubl\n",
      "cat\n",
      "charct\n"
     ]
    }
   ],
   "source": [
    "print(lancaster.stem('changes'))\n",
    "print(lancaster.stem('troubling'))\n",
    "print(lancaster.stem('troubled'))\n",
    "print(lancaster.stem('cats'))\n",
    "print(lancaster.stem('charcterization'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friend               friend               friend\n",
      "friendship           friendship           friend\n",
      "friends              friend               friend\n",
      "friendships          friendship           friend\n",
      "stabil               stabil               stabl\n",
      "destabilize          destabil             dest\n",
      "misunderstanding     misunderstand        misunderstand\n",
      "railroad             railroad             railroad\n",
      "moonlight            moonlight            moonlight\n",
      "football             footbal              footbal\n"
     ]
    }
   ],
   "source": [
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\", \"destabilize\", \"misunderstanding\", \"railroad\", \"moonlight\", \"football\"]\n",
    "\n",
    "for word in word_list:\n",
    "    print(f'{word:20} {porter.stem(word):20} {lancaster.stem(word)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python are veri intellig , and work veri pythonli and now they are python their way to success .'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stems.append(porter.stem(token))\n",
    "    \n",
    "    return \" \".join(stems)\n",
    "\n",
    "sentence = \"Pythoners are very intelligent, and work very pythonly and now they are pythoning their way to success.\"\n",
    "\n",
    "stems = stem_sentence(sentence)\n",
    "stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat\n",
      "foot\n",
      "are\n",
      "change\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('bats'))\n",
    "print(lemmatizer.lemmatize('feet'))\n",
    "print(lemmatizer.lemmatize('are'))\n",
    "print(lemmatizer.lemmatize('changes'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "swim\n",
      "swimming\n",
      "strip\n",
      "stripe\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('are',pos='v'))\n",
    "print(lemmatizer.lemmatize('swimming',pos='v'))\n",
    "print(lemmatizer.lemmatize('swimming',pos='n'))\n",
    "print(lemmatizer.lemmatize('stripes',pos='v'))\n",
    "print(lemmatizer.lemmatize('stripes',pos='n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He is running and eating at the same time. He has bad habit of swimming after playing long hours in the Sun'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentance = 'He is running and eating at the same time. He has bad habit of swimming after playing long hours in the Sun'\n",
    "sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'is',\n",
       " 'running',\n",
       " 'and',\n",
       " 'eating',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'He',\n",
       " 'has',\n",
       " 'bad',\n",
       " 'habit',\n",
       " 'of',\n",
       " 'swimming',\n",
       " 'after',\n",
       " 'playing',\n",
       " 'long',\n",
       " 'hours',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Sun']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(sentance)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He                   He\n",
      "is                   is\n",
      "running              running\n",
      "and                  and\n",
      "eating               eating\n",
      "at                   at\n",
      "the                  the\n",
      "same                 same\n",
      "time                 time\n",
      "He                   He\n",
      "has                  ha\n",
      "bad                  bad\n",
      "habit                habit\n",
      "of                   of\n",
      "swimming             swimming\n",
      "after                after\n",
      "playing              playing\n",
      "long                 long\n",
      "hours                hour\n",
      "in                   in\n",
      "the                  the\n",
      "Sun                  Sun\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(f'{token:20} {lemmatizer.lemmatize(token)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leav\n",
      "leaf\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem('leaves'))\n",
    "print(porter.stem('leafs'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leave\n",
      "leaf\n",
      "leaf\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('leaves', pos='v'))\n",
    "print(lemmatizer.lemmatize('leaves', pos='n'))\n",
    "print(lemmatizer.lemmatize('leafs'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"POS Tagging is a process to mark up the words in text format for a particular part of a speech, based on its definition and context.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POS',\n",
       " 'Tagging',\n",
       " 'is',\n",
       " 'a',\n",
       " 'process',\n",
       " 'to',\n",
       " 'mark',\n",
       " 'up',\n",
       " 'the',\n",
       " 'words',\n",
       " 'in',\n",
       " 'text',\n",
       " 'format',\n",
       " 'for',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'speech',\n",
       " ',',\n",
       " 'based',\n",
       " 'on',\n",
       " 'its',\n",
       " 'definition',\n",
       " 'and',\n",
       " 'context',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('POS', 'Tagging'),\n",
       " ('Tagging', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'process'),\n",
       " ('process', 'to'),\n",
       " ('to', 'mark'),\n",
       " ('mark', 'up'),\n",
       " ('up', 'the'),\n",
       " ('the', 'words'),\n",
       " ('words', 'in'),\n",
       " ('in', 'text'),\n",
       " ('text', 'format'),\n",
       " ('format', 'for'),\n",
       " ('for', 'a'),\n",
       " ('a', 'particular'),\n",
       " ('particular', 'part'),\n",
       " ('part', 'of'),\n",
       " ('of', 'a'),\n",
       " ('a', 'speech'),\n",
       " ('speech', ','),\n",
       " (',', 'based'),\n",
       " ('based', 'on'),\n",
       " ('on', 'its'),\n",
       " ('its', 'definition'),\n",
       " ('definition', 'and'),\n",
       " ('and', 'context'),\n",
       " ('context', '.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = list(nltk.bigrams(tokens))\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('POS', 'Tagging', 'is'),\n",
       " ('Tagging', 'is', 'a'),\n",
       " ('is', 'a', 'process'),\n",
       " ('a', 'process', 'to'),\n",
       " ('process', 'to', 'mark'),\n",
       " ('to', 'mark', 'up'),\n",
       " ('mark', 'up', 'the'),\n",
       " ('up', 'the', 'words'),\n",
       " ('the', 'words', 'in'),\n",
       " ('words', 'in', 'text'),\n",
       " ('in', 'text', 'format'),\n",
       " ('text', 'format', 'for'),\n",
       " ('format', 'for', 'a'),\n",
       " ('for', 'a', 'particular'),\n",
       " ('a', 'particular', 'part'),\n",
       " ('particular', 'part', 'of'),\n",
       " ('part', 'of', 'a'),\n",
       " ('of', 'a', 'speech'),\n",
       " ('a', 'speech', ','),\n",
       " ('speech', ',', 'based'),\n",
       " (',', 'based', 'on'),\n",
       " ('based', 'on', 'its'),\n",
       " ('on', 'its', 'definition'),\n",
       " ('its', 'definition', 'and'),\n",
       " ('definition', 'and', 'context'),\n",
       " ('and', 'context', '.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = list(nltk.trigrams(tokens))\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "dog = wordnet.synsets('dog')[0]\n",
    "cat = wordnet.synsets('cat')[0]\n",
    "play = wordnet.synsets('play')[0]\n",
    "\n",
    "print(dog.wup_similarity(cat))\n",
    "print(dog.wup_similarity(play))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small\n",
      "small\n",
      "small\n",
      "little\n",
      "minor\n",
      "modest\n",
      "small\n",
      "small-scale\n",
      "pocket-size\n",
      "pocket-sized\n",
      "little\n",
      "small\n",
      "small\n",
      "humble\n",
      "low\n",
      "lowly\n",
      "modest\n",
      "small\n",
      "little\n",
      "minuscule\n",
      "small\n",
      "little\n",
      "small\n",
      "small\n",
      "modest\n",
      "small\n",
      "belittled\n",
      "diminished\n",
      "small\n",
      "small\n"
     ]
    }
   ],
   "source": [
    "for ss in wordnet.synsets('small'):\n",
    "    for name in ss.lemma_names():\n",
    "        print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
